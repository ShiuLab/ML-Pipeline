{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workshop\n",
    "\n",
    "Here we will walk through an example of a machine learning workflow following five steps:\n",
    "\n",
    "<img src=\"../_img/ml_workflow.png\" alt=\"ML Workflow\" width=\"800\"/>\n",
    "\n",
    "For more detailed information on the Shiu Lab's ML pipeline, including explanations of all output files,\n",
    "check out the [README](https://github.com/ShiuLab/ML-Pipeline).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Set up Jupyter notebook & software\n",
    "\n",
    "Check out this [**guide**](../Tutorial/README.md) to learn how to set up Jupyter notebook and the software needed to run the Shiu Lab's ML pipeline.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step 1](../_img/step1.png \"ML Workflow step 1\")\n",
    "\n",
    "**What do we want to predict?** \n",
    "\n",
    "If a gene is annotated as being involved in specialized or general metabolism. \n",
    "\n",
    "**What are the labeled instances?**\n",
    "\n",
    "Tomato genes annotated as being involved in specialized or general metabolism by TomatoCyc.\n",
    "\n",
    "**What are the predictive features?** \n",
    "- duplication information (e.g. number of paralogs, gene family size)\n",
    "- sequence conservation (e.g. nonsynonymous/synonymouse substitution rates between homologs)\n",
    "- gene expression (e.g. breadth, stress specific, co-expression)\n",
    "- protein domain conent (e.g. p450, Aldedh)\n",
    "- epigenetic modification (e.g. H3K23ac histone marks)\n",
    "- network properties (# protein-protein interactions, network connectivity).\n",
    "\n",
    "**What data do we have?**\n",
    "- 532 tomato genes with specialized metabolism annotation by TomatoCyc\n",
    "- 2,318 tomato genes with general metabolism annotation by TomatoCyc\n",
    "- 4,197 features (we are only using a subset of **564** for this workshop)\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step 2](../_img/step2.png \"ML Workflow step 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (rows, cols):\n",
      "(2872, 565)\n",
      "\n",
      "Snapshot of data:\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize FamilySize_cat  \\\n",
      "YP_008563134      gen                    0.0    0.010582         medium   \n",
      "XP_010327628      gen                    0.0    0.000000          small   \n",
      "XP_010327620  special                    0.0    0.052910         medium   \n",
      "XP_010327578      gen                    0.0         NaN            NaN   \n",
      "XP_010327494      gen                    1.0    0.021164         medium   \n",
      "YP_008563119  special                    0.0    0.000000          small   \n",
      "\n",
      "              Transferase  \n",
      "YP_008563134          0.0  \n",
      "XP_010327628          NaN  \n",
      "XP_010327620          0.0  \n",
      "XP_010327578          0.0  \n",
      "XP_010327494          0.0  \n",
      "YP_008563119          0.0  \n",
      "\n",
      "List of class labels\n",
      "gen        2318\n",
      "special     532\n",
      "unknown      22\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## A. Lets look at the data (note, you can do this in excel or R!)\n",
    "import pandas as pd\n",
    "\n",
    "d = pd.read_table('data.txt', sep='\\t', index_col = 0)\n",
    "\n",
    "print('Shape of data (rows, cols):')\n",
    "print(d.shape)\n",
    "\n",
    "print('\\nSnapshot of data:')\n",
    "print(d.iloc[:6,:5])  # prints first 6 rows and 5 columns\n",
    "\n",
    "print('\\nList of class labels')\n",
    "print(d['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to notice:**\n",
    "- Our data has NAs. ML algorithms cannot handel NAs. We either needs to drop or impute NA values!\n",
    "- We have binary, continuous, and categorical features in this dataset. A perk of ML models is that they can integrate multiple datatypes in a single model. \n",
    "- However, before being used as input, a categorical feature needs to be converted into set binary features using an approach called [one-hot-encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding). \n",
    "\n",
    "*Before One-Hot Encoding:*\n",
    "\n",
    "| ID   | Class    | Weather   |\n",
    "|---    |---    |---    |\n",
    "| instance_A    |  1     | sunny     |\n",
    "| instance_B    |  0    |  overcast     |\n",
    "| instance_C   |  0     |  rain    | \n",
    "| instance_D   | 1     |  sunny    |\n",
    "\n",
    "*After One-Hot Encoding:*\n",
    "\n",
    "| ID   | Class    | Weather_sunny   | Weather_overcast   | Weather_rain   |\n",
    "|---    |---    |---    |---    |---    |\n",
    "| instance_A    |  1     | 1     | 0     | 0     |\n",
    "| instance_B    |  0    |  0     |  1     |  0     |\n",
    "| instance_C   |  0     |  0    |  0    |  1    | \n",
    "| instance_D   | 1     |  1    | 0    | 0    |\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated data cleaning: ML_preprocess.py\n",
    "\n",
    "Input\n",
    "```\n",
    "-df: your data table\n",
    "-na_method: how you want to impute NAs (options: drop, mean, median, mode)\n",
    "-h: show more options\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot of input data...\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize FamilySize_cat  \\\n",
      "YP_008563134      gen                    0.0    0.010582         medium   \n",
      "XP_010327628      gen                    0.0    0.000000          small   \n",
      "XP_010327620  special                    0.0    0.052910         medium   \n",
      "XP_010327578      gen                    0.0         NaN            NaN   \n",
      "XP_010327494      gen                    1.0    0.021164         medium   \n",
      "\n",
      "              Transferase  \n",
      "YP_008563134          0.0  \n",
      "XP_010327628          NaN  \n",
      "XP_010327620          0.0  \n",
      "XP_010327578          0.0  \n",
      "XP_010327494          0.0  \n",
      "\n",
      "\n",
      "### Dropping/imputing NAs... ###\n",
      "\n",
      "Number of columns with NAs: 41\n",
      "Features dropped because missing > 50.00% of data: ['SQS_PSY']\n",
      "Number of columns to impute: 40\n",
      "\n",
      "\n",
      "### One Hot Encoding... ###\n",
      "\n",
      "Features to one-hot-encode: ['FamilySize_cat']\n",
      "Dataframe shape (rows, cols) before and after one-hot-encoding:\n",
      "Before: (2872, 563)\n",
      "After: (2872, 565)\n",
      "\n",
      "Number of duplicate row names to delete: 0\n",
      "\n",
      "Snapshot of imputed data...\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      gen                    0.0    0.010582          0.0   \n",
      "XP_010327628      gen                    0.0    0.000000          0.0   \n",
      "XP_010327620  special                    0.0    0.052910          0.0   \n",
      "XP_010327578      gen                    0.0    0.015873          0.0   \n",
      "XP_010327494      gen                    1.0    0.021164          0.0   \n",
      "\n",
      "              Exo_endo_phos  \n",
      "YP_008563134            0.0  \n",
      "XP_010327628            0.0  \n",
      "XP_010327620            0.0  \n",
      "XP_010327578            0.0  \n",
      "XP_010327494            0.0  \n",
      "\n",
      "Output file saved as: data_mod.txt\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# B. Drop/Impute NAs and one-hot-encode categorical features\n",
    "\n",
    "%run ../ML_preprocess.py -df data.txt -na_method median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Set aside instances for testing \n",
    "\n",
    "We want to set aside a subset of our data to use to test how well our model performed. Note that this is done before feature engineering, parameter selection, or model training. This will ensure our performance metric is entirely independent from our modeling!\n",
    "\n",
    "\n",
    "### Automated selection of test set: test_set.py\n",
    "\n",
    "Input\n",
    "```\n",
    "-df: your data table\n",
    "-use: what class labels to include in the test set (we don't want to include unknowns!)\n",
    "-type: (c) classification or (r) regression\n",
    "-p: What percent of instances from each class to select for test (0.1 = 10%)\n",
    "-save: save name for test set\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holding out 10.0 percent\n",
      "Pulling test set from classes: ['gen', 'special']\n",
      "285 instances in test set\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "# C. Define test set\n",
    "\n",
    "%run ../test_set.py -df data_mod.txt  \\\n",
    "                    -use gen,special  \\\n",
    "                    -type c  \\\n",
    "                    -p 0.1  \\\n",
    "                    -save test_genes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "![Step 3](../_img/step3.png \"ML Workflow step 3\")\n",
    "\n",
    "While one major advantage of ML approaches is that they are robust when the number of features is very large, there are cases where removing unuseful features or selecting only the best features may help you better answer your question. One common issue we see with using feature selection for machine learning is using the whole dataset to select the best features, which results in overfitting! **Be sure you specify your test set so that this data is not used for feature selection!**\n",
    "\n",
    "\n",
    "### Automated feature selection: Feature_Selection.py\n",
    "\n",
    "Input\n",
    "```\n",
    "-df: your data table\n",
    "-test: what instances to hold out (i.e. test instances!)\n",
    "-cl_train: labels to include in training the feature selection algorithm\n",
    "-type: (c) classification or (r) regression\n",
    "-alg: what feature selection algorithm to use (e.g. lasso, elastic net, random forest)\n",
    "-p: Parameter specific to different algorithms (use -h for more information)\n",
    "-n: Number of feature to select (unless algorithm does this automatically)\n",
    "-save: save name for list of selected features\n",
    "```\n",
    "\n",
    "\n",
    "Here we will use one of the most common feature selection algorithms: LASSO. LASSO requires the user to select the level of sparcity (-p) they want to induce during feature selection, where a larger value will result in more features being selected and a smaller value resulting in fewer features being selected. You can play around with this value to see what it does for your data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing testldout instances...\n",
      "Dropping instances that are not in ['special', 'gen'], changed dimensions from (2587, 566) to (2565, 566) (instance, features).\n",
      "\n",
      "Snapshot of data:\n",
      "              Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      0                    0.0    0.010582          0.0   \n",
      "XP_010327628      0                    0.0    0.000000          0.0   \n",
      "XP_010327578      0                    0.0    0.015873          0.0   \n",
      "XP_010327494      0                    1.0    0.021164          0.0   \n",
      "YP_008563119      1                    0.0    0.000000          0.0   \n",
      "YP_008563115      0                    0.0    0.010582          0.0   \n",
      "\n",
      "              Exo_endo_phos  \n",
      "YP_008563134            0.0  \n",
      "XP_010327628            0.0  \n",
      "XP_010327578            0.0  \n",
      "XP_010327494            0.0  \n",
      "YP_008563119            0.0  \n",
      "YP_008563115            0.0  \n",
      "=====* Running L1/LASSO based feature selection *=====\n",
      "Features selected using LASSO: ['p450' 'UDPGT' 'tandemDupGenes' 'Nicotiana_tabacum.TN90_AYMY.SS.csv'\n",
      " 'Ppatens_318_v3.3.csv' 'Atrichopoda_291_v1.0.csv' 'Coffea_canephora.csv'\n",
      " 'Nicotiana_tomen.csv' 'BrapaFPsc_277_v1.3.csv' 'FamilySize_cat_small']\n",
      "\n",
      "Number of features selected using LASSO (sparcity parameter = 0.01): 10\n",
      "Run time (sec):0.33\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%run ../Feature_Selection.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen  \\\n",
    "                            -type c  \\\n",
    "                            -alg lasso  \\\n",
    "                            -p 0.01  \\\n",
    "                            -save top_feat_lasso.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing testldout instances...\n",
      "Dropping instances that are not in ['special', 'gen'], changed dimensions from (2587, 566) to (2565, 566) (instance, features).\n",
      "\n",
      "Snapshot of data:\n",
      "              Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      0                    0.0    0.010582          0.0   \n",
      "XP_010327628      0                    0.0    0.000000          0.0   \n",
      "XP_010327620      1                    0.0    0.052910          0.0   \n",
      "XP_010327578      0                    0.0    0.015873          0.0   \n",
      "XP_010327494      0                    1.0    0.021164          0.0   \n",
      "YP_008563115      0                    0.0    0.010582          0.0   \n",
      "\n",
      "              Exo_endo_phos  \n",
      "YP_008563134            0.0  \n",
      "XP_010327628            0.0  \n",
      "XP_010327620            0.0  \n",
      "XP_010327578            0.0  \n",
      "XP_010327494            0.0  \n",
      "YP_008563115            0.0  \n",
      "Run time (sec):0.28\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%run ../Feature_Selection.py -df data_mod.txt  \\\n",
    "                            -test test_genes.txt  \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -type c  \\\n",
    "                            -alg random  \\\n",
    "                            -n 10  \\\n",
    "                            -save rand_feat.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "![Step 4](../_img/step4.png \"ML Workflow step 4\")\n",
    "\n",
    "Next we want to determine which ML algorithm (i.e. Support Vector Machine (SVM), Random Forest (RF)) we should use and what parameters needed by those algorithms work best. Importantly, at this stage we **only assess our model performance on the validation data** in order to assure we aren't just selecting the algorithm that works best on our held out testing data. The pipeline will automatically withhold the testing data from the parameter selection (i.e. grid search) step. \n",
    "\n",
    "Note, the pipeline **automatically \"balances\" your data**, meaning it pulls the same number of instances of each class for training. This avoids biasing the model to just predict everything as the more common class. This is a major reason why we want to run multiple replicates of the model!\n",
    "\n",
    "\n",
    "### Algorithm Selection\n",
    "The machine learning algorithms in the ML_Pipeline are implement from [SciKit-Learn](https://scikit-learn.org/stable/), which has excellent resources to learn more about the ins and outs of these algorithms.\n",
    "\n",
    "**Why is algorithm selection useful?** ML models are able to learn patterns from data without the being explictely programmed to look for those patterns. ML algorithms differ in what patterns they excel at finding. For example, SVM is limited to linear relationships between feature and labels, while RF, because of its heiarchical structure, is able to model interactive patterns between your features. Furthermore, algorithms vary in their complexity and the amount of training data that is needed in order to  \n",
    "\n",
    "\n",
    "### Parameter Selection\n",
    "Most ML algorithms have internal parameters that need to be set by the user. For example:\n",
    "\n",
    "![RF Parameter examples](../_img/rf_params.png \"Sample of RF parameters\")\n",
    "\n",
    "\n",
    "There are two general strategies for parameter selection: the grid search (default option: left) and the random search (use \"-gs_type random\": right):\n",
    "![Grid search](../_img/grid_rand_search.png \"Grid Search\")\n",
    "*Image: Bergstra & Bengio 2012; used under CC-BY license*\n",
    "\n",
    "\n",
    "### Training and Validation\n",
    "Training and validation is done using a [cross-validation (CV)](https://towardsdatascience.com/cross-validation-70289113a072) scheme. CV is useful because it makes good use of our data (i.e. uses all non-test data for training at some point) but also makes sure we are selecting the best parameters/algorithms on models that aren't overfit to the training data. Here is a visual to demonstrate how CV works (with 10-cv folds in this example):\n",
    "\n",
    "![Cross Validation](../_img/cross_validation.png \"Cross validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated parameter selection, ML training, validation, & testing:  ML_classification.py/ML_regression.py\n",
    "\n",
    "Input\n",
    "```\n",
    "-df: your data table\n",
    "-test: what instances to hold out (i.e. test instances)\n",
    "-cl_train: labels to include in training the feature selection algorithm\n",
    "-alg: what ML algorithm to use (e.g. SVM, RF, LogReg)\n",
    "-cv: Number of cross-validation folds (default = 10, use fewer if data set is small)\n",
    "-n: Number of replicates of the cross-validation scheme to run (default = 100)\n",
    "```\n",
    "\n",
    "*There are many functions available within the pipeline that are not described in this workshop. For more options run:*\n",
    "```\n",
    "python ML_classification.py -h\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test instances to apply model on later...\n",
      "Snapshot of data being used:\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      gen                    0.0    0.016667          0.0   \n",
      "XP_010327628      gen                    0.0    0.000000          0.0   \n",
      "XP_010327620  special                    0.0    0.083333          0.0   \n",
      "XP_010327578      gen                    0.0    0.025000          0.0   \n",
      "XP_010327494      gen                    1.0    0.033333          0.0   \n",
      "\n",
      "              Exo_endo_phos  \n",
      "YP_008563134            0.0  \n",
      "XP_010327628            0.0  \n",
      "XP_010327620            0.0  \n",
      "XP_010327578            0.0  \n",
      "XP_010327494            0.0  \n",
      "\n",
      "\n",
      "CLASSES: ['gen' 'special']\n",
      "POS: special type:  <class 'str'>\n",
      "NEG: gen type:  <class 'str'>\n",
      "\n",
      "Balanced dataset will include 478 instances of each class\n",
      "\n",
      "\n",
      "===>  Grid search started  <===\n",
      "Round 1 of 10\n",
      "Round 2 of 10\n",
      "Round 3 of 10\n",
      "Round 4 of 10\n",
      "Round 5 of 10\n",
      "Round 6 of 10\n",
      "Round 7 of 10\n",
      "Round 8 of 10\n",
      "Round 9 of 10\n",
      "Round 10 of 10\n",
      "Parameter sweep time: 20.410971 seconds\n",
      "Parameters selected: Kernel=Linear, C=0.5\n",
      "Grid search complete. Time: 20.441754 seconds\n",
      "\n",
      "\n",
      "===>  ML Pipeline started  <===\n",
      "  Round 1 of 10\n",
      "  Round 2 of 10\n",
      "  Round 3 of 10\n",
      "  Round 4 of 10\n",
      "  Round 5 of 10\n",
      "  Round 6 of 10\n",
      "  Round 7 of 10\n",
      "  Round 8 of 10\n",
      "  Round 9 of 10\n",
      "  Round 10 of 10\n",
      "ML Pipeline time: 11.699244 seconds\n",
      "\n",
      "\n",
      "===>  ML Results  <===\n",
      "\n",
      "Validation Set Scores\n",
      "Accuracy: 0.843828 (+/- stdev 0.005862)\n",
      "F1: 0.848847 (+/- stdev 0.005386)\n",
      "AUC-ROC: 0.908589 (+/- stdev 0.004616)\n",
      "AUC-PRC: 0.905385 (+/- stdev 0.008574)\n",
      "\n",
      "\n",
      "Test Set Scores:\n",
      "Precision: 0.823875\n",
      "Accuracy: 0.846234\n",
      "F1: 0.851365\n",
      "AUC-ROC: 0.931953 (+/- stdev 0.004301)\n",
      "AUC-PRC: 0.855205 (+/- stdev 0.011839)\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                        -test test_genes.txt \\\n",
    "                        -cl_train special,gen \\\n",
    "                        -alg SVM \\\n",
    "                        -cv 5 \\\n",
    "                        -n 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Breakdown\n",
    "\n",
    "There are dozens of [performance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) that can be used to assess how well a ML model works. While the best metric for you depends on the type of question you are asking, some of the most generally useful metrics include the area under the Receiver Operator Characteristic curve (AUC-ROC), the area under the Precision-Recall curve (AUC_PRc), and the F-measure (F1).\n",
    "\n",
    "![AUCROC_Correlation](../_img/metrics.png \"AUCROC Correlation\")\n",
    "\n",
    "\n",
    "Running the same script (only changing **-alg XXX**), average performance on the validation data using other algorithms:\n",
    "\n",
    "| Alg  \t| F1  \t| AUC-ROC  \t|\n",
    "|---\t|---\t|---\t|\n",
    "| RF  \t| 0.787  \t| 0.824  \t|\n",
    "| LogReg  \t| 0.862  \t| 0.921  \t|\n",
    "| SVMpoly  \t| 0.833  \t| 0.897  \t|\n",
    "| SVMrbf  \t| 0.855  \t| 0.905  \t|\n",
    "| SVM  \t| 0.856  \t| 0.911  \t|\n",
    "\n",
    "\n",
    "***SVM performed best on the validation data so we will continue with that algorithm!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step 5](../_img/step5.png \"ML Workflow step 5\")\n",
    "\n",
    "Now that we have our best performing algorithm, we will run the pipeline one more time, but with more replicates and we will use it to predict our unknown genes. \n",
    "\n",
    "Additional input:\n",
    "```\n",
    "- apply: List of lable names to apply trained model to (i.e. all, or 'unknown')\n",
    "- plots: True/False if you want the pipeline to generate performance metric plots (default = F)\n",
    "- save: Name to save output to (will over-write old files)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test instances to apply model on later...\n",
      "Snapshot of data being used:\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      gen                    0.0    0.016667          0.0   \n",
      "XP_010327628      gen                    0.0    0.000000          0.0   \n",
      "XP_010327620  special                    0.0    0.083333          0.0   \n",
      "XP_010327578      gen                    0.0    0.025000          0.0   \n",
      "XP_010327494      gen                    1.0    0.033333          0.0   \n",
      "\n",
      "              Exo_endo_phos  \n",
      "YP_008563134            0.0  \n",
      "XP_010327628            0.0  \n",
      "XP_010327620            0.0  \n",
      "XP_010327578            0.0  \n",
      "XP_010327494            0.0  \n",
      "\n",
      "\n",
      "CLASSES: ['gen' 'special']\n",
      "POS: special type:  <class 'str'>\n",
      "NEG: gen type:  <class 'str'>\n",
      "\n",
      "Balanced dataset will include 478 instances of each class\n",
      "\n",
      "\n",
      "===>  Grid search started  <===\n",
      "Round 1 of 10\n",
      "Round 2 of 10\n",
      "Round 3 of 10\n",
      "Round 4 of 10\n",
      "Round 5 of 10\n",
      "Round 6 of 10\n",
      "Round 7 of 10\n",
      "Round 8 of 10\n",
      "Round 9 of 10\n",
      "Round 10 of 10\n",
      "Parameter sweep time: 21.742878 seconds\n",
      "Parameters selected: Kernel=Linear, C=0.5\n",
      "Grid search complete. Time: 21.747271 seconds\n",
      "\n",
      "\n",
      "===>  ML Pipeline started  <===\n",
      "  Round 1 of 50\n",
      "  Round 2 of 50\n",
      "  Round 3 of 50\n",
      "  Round 4 of 50\n",
      "  Round 5 of 50\n",
      "  Round 6 of 50\n",
      "  Round 7 of 50\n",
      "  Round 8 of 50\n",
      "  Round 9 of 50\n",
      "  Round 10 of 50\n",
      "  Round 11 of 50\n",
      "  Round 12 of 50\n",
      "  Round 13 of 50\n",
      "  Round 14 of 50\n",
      "  Round 15 of 50\n",
      "  Round 16 of 50\n",
      "  Round 17 of 50\n",
      "  Round 18 of 50\n",
      "  Round 19 of 50\n",
      "  Round 20 of 50\n",
      "  Round 21 of 50\n",
      "  Round 22 of 50\n",
      "  Round 23 of 50\n",
      "  Round 24 of 50\n",
      "  Round 25 of 50\n",
      "  Round 26 of 50\n",
      "  Round 27 of 50\n",
      "  Round 28 of 50\n",
      "  Round 29 of 50\n",
      "  Round 30 of 50\n",
      "  Round 31 of 50\n",
      "  Round 32 of 50\n",
      "  Round 33 of 50\n",
      "  Round 34 of 50\n",
      "  Round 35 of 50\n",
      "  Round 36 of 50\n",
      "  Round 37 of 50\n",
      "  Round 38 of 50\n",
      "  Round 39 of 50\n",
      "  Round 40 of 50\n",
      "  Round 41 of 50\n",
      "  Round 42 of 50\n",
      "  Round 43 of 50\n",
      "  Round 44 of 50\n",
      "  Round 45 of 50\n",
      "  Round 46 of 50\n",
      "  Round 47 of 50\n",
      "  Round 48 of 50\n",
      "  Round 49 of 50\n",
      "  Round 50 of 50\n",
      "ML Pipeline time: 52.107224 seconds\n",
      "\n",
      "Generating ROC & PR curves\n"
     ]
    }
   ],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -alg SVM \\\n",
    "                            -cv 5 \\\n",
    "                            -n 50 \\\n",
    "                            -apply unknown \\\n",
    "                            -plots T \\\n",
    "                            -save metab_SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check out our results...**\n",
    "\n",
    "Here are the files that are output from the model:\n",
    "- **data.txt_results:** A detailed look at the model that was run and its performance.  \n",
    "\n",
    "- **data.txt_scores:** The probability score for each gene (i.e. how confidently it was predicted) and the final classification for each gene, including the unknowns the model was applied to.\n",
    "\n",
    "- **data.txt_imp:** The importance of each feature in your model.\n",
    "\n",
    "- **data.txt_GridSearch:** Detailed results from the parameter grid search.\n",
    "\n",
    "- **data.txt_BalancedID:** A list of the genes that were included in each replicate after downsampling to balance the model.\n",
    "\n",
    "*For a detailed description of the content of the pipeline output see the [README](../README.md)*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we use fewer features?\n",
    "\n",
    "Additional input:\n",
    "```\n",
    "- feat: List of features to use.\n",
    "```\n",
    "Use smaller balenced data set\n",
    "\n",
    "### Advanced Topics\n",
    "- multiclass\n",
    "- transfer learning\n",
    "- venn diagrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset of features from: top_feat_lasso.txt\n",
      "Removing test instances to apply model on later...\n",
      "Snapshot of data being used:\n",
      "                Class  p450  UDPGT  tandemDupGenes  \\\n",
      "YP_008563134      gen   0.0    0.0             0.0   \n",
      "XP_010327628      gen   0.0    0.0             0.0   \n",
      "XP_010327620  special   0.0    0.0             0.0   \n",
      "XP_010327578      gen   0.0    0.0             0.0   \n",
      "XP_010327494      gen   0.0    0.0             0.0   \n",
      "\n",
      "              Nicotiana_tabacum.TN90_AYMY.SS.csv  \n",
      "YP_008563134                                 0.0  \n",
      "XP_010327628                                 0.0  \n",
      "XP_010327620                                 0.0  \n",
      "XP_010327578                                 0.0  \n",
      "XP_010327494                                 1.0  \n",
      "\n",
      "\n",
      "CLASSES: ['gen' 'special']\n",
      "POS: special type:  <class 'str'>\n",
      "NEG: gen type:  <class 'str'>\n",
      "\n",
      "Balanced dataset will include 478 instances of each class\n",
      "\n",
      "\n",
      "===>  Grid search started  <===\n",
      "Round 1 of 10\n",
      "Round 2 of 10\n",
      "Round 3 of 10\n",
      "Round 4 of 10\n",
      "Round 5 of 10\n",
      "Round 6 of 10\n",
      "Round 7 of 10\n",
      "Round 8 of 10\n",
      "Round 9 of 10\n",
      "Round 10 of 10\n",
      "Parameter sweep time: 6.891783 seconds\n",
      "Parameters selected: Kernel=Linear, C=0.5\n",
      "Grid search complete. Time: 6.894049 seconds\n",
      "\n",
      "\n",
      "===>  ML Pipeline started  <===\n",
      "  Round 1 of 10\n",
      "  Round 2 of 10\n",
      "  Round 3 of 10\n",
      "  Round 4 of 10\n",
      "  Round 5 of 10\n",
      "  Round 6 of 10\n",
      "  Round 7 of 10\n",
      "  Round 8 of 10\n",
      "  Round 9 of 10\n",
      "  Round 10 of 10\n",
      "ML Pipeline time: 4.010258 seconds\n",
      "\n",
      "\n",
      "===>  ML Results  <===\n",
      "\n",
      "Validation Set Scores\n",
      "Accuracy: 0.635356 (+/- stdev 0.016613)\n",
      "F1: 0.703515 (+/- stdev 0.009513)\n",
      "AUC-ROC: 0.736579 (+/- stdev 0.009872)\n",
      "AUC-PRC: 0.736462 (+/- stdev 0.011852)\n",
      "\n",
      "\n",
      "Test Set Scores:\n",
      "Precision: 0.599419\n",
      "Accuracy: 0.643305\n",
      "F1: 0.707798\n",
      "AUC-ROC: 0.700146 (+/- stdev 0.007866)\n",
      "AUC-PRC: 0.382885 (+/- stdev 0.015616)\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -alg SVM \\\n",
    "                            -cv 5 \\\n",
    "                            -n 10 \\\n",
    "                            -feat top_feat_lasso.txt \\\n",
    "                            -save metab_SVM_lasso10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset of features from: rand_feat.txt_10\n",
      "Removing test instances to apply model on later...\n",
      "Snapshot of data being used:\n",
      "                Class  Osativa_medKaKs  Glyco_hydro_28  NIR_SIR  \\\n",
      "YP_008563134      gen         0.247338             0.0      0.0   \n",
      "XP_010327628      gen         0.247338             0.0      0.0   \n",
      "XP_010327620  special         0.206369             0.0      0.0   \n",
      "XP_010327578      gen         0.212135             0.0      0.0   \n",
      "XP_010327494      gen         0.179797             0.0      0.0   \n",
      "\n",
      "              GHMP_kinases_N  \n",
      "YP_008563134             0.0  \n",
      "XP_010327628             0.0  \n",
      "XP_010327620             0.0  \n",
      "XP_010327578             0.0  \n",
      "XP_010327494             0.0  \n",
      "\n",
      "\n",
      "CLASSES: ['gen' 'special']\n",
      "POS: special type:  <class 'str'>\n",
      "NEG: gen type:  <class 'str'>\n",
      "\n",
      "Balanced dataset will include 478 instances of each class\n",
      "\n",
      "\n",
      "===>  Grid search started  <===\n",
      "Round 1 of 10\n",
      "Round 2 of 10\n",
      "Round 3 of 10\n",
      "Round 4 of 10\n",
      "Round 5 of 10\n",
      "Round 6 of 10\n",
      "Round 7 of 10\n",
      "Round 8 of 10\n",
      "Round 9 of 10\n",
      "Round 10 of 10\n",
      "Parameter sweep time: 5.684607 seconds\n",
      "Parameters selected: Kernel=Linear, C=50.0\n",
      "Grid search complete. Time: 5.686240 seconds\n",
      "\n",
      "\n",
      "===>  ML Pipeline started  <===\n",
      "  Round 1 of 10\n",
      "  Round 2 of 10\n",
      "  Round 3 of 10\n",
      "  Round 4 of 10\n",
      "  Round 5 of 10\n",
      "  Round 6 of 10\n",
      "  Round 7 of 10\n",
      "  Round 8 of 10\n",
      "  Round 9 of 10\n",
      "  Round 10 of 10\n",
      "ML Pipeline time: 9.015551 seconds\n",
      "\n",
      "\n",
      "===>  ML Results  <===\n",
      "\n",
      "Validation Set Scores\n",
      "Accuracy: 0.611925 (+/- stdev 0.021897)\n",
      "F1: 0.696840 (+/- stdev 0.009461)\n",
      "AUC-ROC: 0.655158 (+/- stdev 0.014269)\n",
      "AUC-PRC: 0.587776 (+/- stdev 0.013489)\n",
      "\n",
      "\n",
      "Test Set Scores:\n",
      "Precision: 0.541878\n",
      "Accuracy: 0.569038\n",
      "F1: 0.674566\n",
      "AUC-ROC: 0.639915 (+/- stdev 0.001819)\n",
      "AUC-PRC: 0.265566 (+/- stdev 0.000393)\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -alg SVM \\\n",
    "                            -cv 5 \\\n",
    "                            -n 10 \\\n",
    "                            -feat rand_feat.txt_10 \\\n",
    "                            -save metab_SVM_rand10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Results\n",
    "\n",
    "There are a number of vizualization tools available in the ML-Pipeline (see ML_Postprocessing). Here we will use ML_plots. \n",
    "\n",
    "\n",
    "**ML_plots.py input:**\n",
    "```\n",
    "ML_plots.py [SAVE_NAME] [POS] [NEG] [M1_name] [PATH_M1_scores] [M2_name] [PATH_M2_scores]... [Mn_name] [PATH_Mn_scores]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_plots.py compare_SVM special gen \\ \n",
    "                    all metab_SVM_scores.txt \\\n",
    "                    LASSO metab_lasso10_SVM_scores.txt\n",
    "                    Random metab_rand10_SVM_scores.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
