{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workshop\n",
    "\n",
    "Here we will walk through an example of a machine learning workflow following five steps:\n",
    "\n",
    "<img src=\"../_img/ml_workflow.png\" alt=\"ML Workflow\" width=\"800\"/>\n",
    "\n",
    "For more detailed information on the Shiu Lab's ML pipeline, including explanations of all output files,\n",
    "check out the [README](https://github.com/ShiuLab/ML-Pipeline).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Set up Jupyter notebook & software\n",
    "\n",
    "Check out this [**guide**](https://github.com/ShiuLab/ML-Pipeline/tree/master/Workshop) to learn how to set up Jupyter notebook and the software needed to run the Shiu Lab's ML pipeline.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step 1](../_img/step1.png \"ML Workflow step 1\")\n",
    "\n",
    "**What do we want to predict?** \n",
    "\n",
    "If a gene is annotated as being involved in specialized or general metabolism. \n",
    "\n",
    "**What are the labeled instances?**\n",
    "\n",
    "Tomato genes annotated as being involved in specialized or general metabolism by TomatoCyc.\n",
    "\n",
    "**What are the predictive features?** \n",
    "- duplication information (e.g. number of paralogs, gene family size)\n",
    "- sequence conservation (e.g. nonsynonymous/synonymouse substitution rates between homologs)\n",
    "- gene expression (e.g. breadth, stress specific, co-expression)\n",
    "- protein domain conent (e.g. p450, Aldedh)\n",
    "- epigenetic modification (e.g. H3K23ac histone marks)\n",
    "- network properties (# protein-protein interactions, network connectivity).\n",
    "\n",
    "**What data do we have?**\n",
    "- 532 tomato genes with specialized metabolism annotation by TomatoCyc\n",
    "- 2,318 tomato genes with general metabolism annotation by TomatoCyc\n",
    "- 4,197 features (we are only using a subset of **564** for this workshop)\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step 2](../_img/step2.png \"ML Workflow step 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## A. Lets look at the data (note, you can do this in excel or R!)\n",
    "import pandas as pd\n",
    "\n",
    "d = pd.read_table('data.txt', sep='\\t', index_col = 0)\n",
    "\n",
    "print('Shape of data (rows, cols):')\n",
    "print(d.shape)\n",
    "\n",
    "print('\\nSnapshot of data:')\n",
    "print(d.iloc[:6,:5])  # prints first 6 rows and 5 columns\n",
    "\n",
    "print('\\nList of class labels')\n",
    "print(d['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to notice:**\n",
    "- Our data has NAs. ML algorithms cannot handel NAs. We either needs to drop or impute NA values!\n",
    "- We have binary, continuous, and categorical features in this dataset. A perk of ML models is that they can integrate multiple datatypes in a single model. \n",
    "- However, before being used as input, a categorical feature needs to be converted into set binary features using an approach called [one-hot-encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding). \n",
    "\n",
    "*Before One-Hot Encoding:*\n",
    "\n",
    "| ID   | Class    | Weather   |\n",
    "|---    |---    |---    |\n",
    "| instance_A    |  1     | sunny     |\n",
    "| instance_B    |  0    |  overcast     |\n",
    "| instance_C   |  0     |  rain    | \n",
    "| instance_D   | 1     |  sunny    |\n",
    "\n",
    "*After One-Hot Encoding:*\n",
    "\n",
    "| ID   | Class    | Weather_sunny   | Weather_overcast   | Weather_rain   |\n",
    "|---    |---    |---    |---    |---    |\n",
    "| instance_A    |  1     | 1     | 0     | 0     |\n",
    "| instance_B    |  0    |  0     |  1     |  0     |\n",
    "| instance_C   |  0     |  0    |  0    |  1    | \n",
    "| instance_D   | 1     |  1    | 0    | 0    |\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated data cleaning: ML_preprocess.py\n",
    "\n",
    "Input\n",
    "```\n",
    "-df: your data table\n",
    "-na_method: how you want to impute NAs (options: drop, mean, median, mode)\n",
    "-h: show more options\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Drop/Impute NAs and one-hot-encode categorical features\n",
    "\n",
    "%run ../ML_preprocess.py -df data.txt -na_method median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Set aside instances for testing \n",
    "\n",
    "We want to set aside a subset of our data to use to test how well our model performed. Note that this is done before feature engineering, parameter selection, or model training. This will ensure our performance metric is entirely independent from our modeling!\n",
    "\n",
    "\n",
    "### Automated selection of test set: test_set.py\n",
    "\n",
    "Input:\n",
    "```\n",
    "-df: your data table\n",
    "-use: what class labels to include in the test set (we don't want to include unknowns!)\n",
    "-type: (c) classification or (r) regression\n",
    "-p: What percent of instances from each class to select for test (0.1 = 10%)\n",
    "-save: save name for test set\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Define test set\n",
    "\n",
    "%run ../test_set.py -df data_mod.txt  \\\n",
    "                    -use gen,special  \\\n",
    "                    -type c  \\\n",
    "                    -p 0.1  \\\n",
    "                    -save test_genes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "![Step 3](../_img/step3.png \"ML Workflow step 3\")\n",
    "\n",
    "While one major advantage of ML approaches is that they are robust when the number of features is very large, there are cases where removing unuseful features or selecting only the best features may help you better answer your question. One common issue we see with using feature selection for machine learning is using the whole dataset to select the best features, which results in overfitting! **Be sure you specify your test set so that this data is not used for feature selection!**\n",
    "\n",
    "\n",
    "### Automated feature selection: Feature_Selection.py\n",
    "\n",
    "Input\n",
    "```\n",
    "-df: your data table\n",
    "-test: what instances to hold out (i.e. test instances!)\n",
    "-cl_train: labels to include in training the feature selection algorithm\n",
    "-type: (c) classification or (r) regression\n",
    "-alg: what feature selection algorithm to use (e.g. lasso, elastic net, random forest)\n",
    "-p: Parameter specific to different algorithms (use -h for more information)\n",
    "-n: Number of feature to select (unless algorithm does this automatically)\n",
    "-save: save name for list of selected features\n",
    "```\n",
    "\n",
    "\n",
    "Here we will use one of the most common feature selection algorithms: LASSO. LASSO requires the user to select the level of sparcity (-p) they want to induce during feature selection, where a larger value will result in more features being selected and a smaller value resulting in fewer features being selected. You can play around with this value to see what it does for your data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Feature_Selection.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen  \\\n",
    "                            -type c  \\\n",
    "                            -alg lasso  \\\n",
    "                            -p 0.01  \\\n",
    "                            -save top_feat_lasso.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Feature_Selection.py -df data_mod.txt  \\\n",
    "                            -test test_genes.txt  \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -type c  \\\n",
    "                            -alg random  \\\n",
    "                            -n 10  \\\n",
    "                            -save rand_feat.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "![Step 4](../_img/step4.png \"ML Workflow step 4\")\n",
    "\n",
    "Next we want to determine which ML algorithm we should use and what combination of hyperparameters for those algorithms work best. Importantly, at this stage we **only assess our model performance on the validation data** in order to assure we aren't just selecting the algorithm that works best on our held out testing data. The pipeline will automatically withhold the testing data from the parameter selection (i.e. grid search) step. \n",
    "\n",
    "Note, the pipeline **automatically \"balances\" your data**, meaning it pulls the same number of instances of each class for training. This avoids biasing the model to just predict everything as the more common class. This is a major reason why we want to run multiple replicates of the model!\n",
    "\n",
    "\n",
    "### Algorithm Selection\n",
    "The machine learning algorithms in the ML_Pipeline are implement from [SciKit-Learn](https://scikit-learn.org/stable/), which has excellent resources to learn more about the ins and outs of these algorithms.\n",
    "\n",
    "**Why is algorithm selection useful?** ML models are able to learn patterns from data without the being explictely programmed to look for those patterns. ML algorithms differ in what patterns they excel at finding. For example, SVM is limited to linear relationships between feature and labels, while RF, because of its heiarchical structure, is able to model interactive patterns between your features. Furthermore, algorithms vary in their complexity and the amount of training data that is needed in order to  \n",
    "\n",
    "\n",
    "### [Hyper]-Parameter Selection\n",
    "Most ML algorithms have internal parameters that need to be set by the user. For example:\n",
    "\n",
    "![RF Parameter examples](../_img/rf_params.png \"Description of select of RF parameters\")\n",
    "\n",
    "\n",
    "There are two general strategies for parameter selection: the grid search (default option: left) and the random search (use \"-gs_type random\": right):\n",
    "![Grid search](../_img/grid_rand_search.png \"Grid Search\")\n",
    "*Image: Bergstra & Bengio 2012; used under CC-BY license*\n",
    "\n",
    "\n",
    "### Automated Training and Validation\n",
    "Training and validation is done using a [cross-validation (CV)](https://towardsdatascience.com/cross-validation-70289113a072) scheme. CV is useful because it makes good use of our data (i.e. uses all non-test data for training at some point) but also makes sure we are selecting the best parameters/algorithms on models that aren't overfit to the training data. Here is a visual to demonstrate how CV works (with 10-cv folds in this example):\n",
    "\n",
    "![Cross Validation](../_img/cross_validation.png \"Cross validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML_classification.py (similar to ML_regression.py)\n",
    "\n",
    "**Input:**\n",
    "```\n",
    "-df: your data table\n",
    "-test: what instances to hold out (i.e. test instances)\n",
    "-cl_train: labels to include in training the feature selection algorithm\n",
    "-alg: what ML algorithm to use (e.g. SVM, RF, LogReg (classification only), LR (regression only))\n",
    "-cv: Number of cross-validation folds (default = 10, use fewer if data set is small)\n",
    "-n: Number of replicates of the balanced cross-validation scheme to run (default = 100)\n",
    "-save: Name to save output to (will over-write old files)\n",
    "\n",
    "```\n",
    "\n",
    "*There are many functions available within the pipeline that are not described in this workshop. For more options run:*\n",
    "```\n",
    "python ML_classification.py -h\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                        -test test_genes.txt \\\n",
    "                        -cl_train special,gen \\\n",
    "                        -alg SVM \\\n",
    "                        -cv 5 \\\n",
    "                        -n 10 \\\n",
    "                        -save metab_SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Breakdown\n",
    "\n",
    "There are dozens of [performance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) that can be used to assess how well a ML model works. While the best metric for you depends on the type of question you are asking, some of the most generally useful metrics include the area under the Receiver Operator Characteristic curve (AUC-ROC), the area under the Precision-Recall curve (AUC_PRc), and the F-measure (F1).\n",
    "\n",
    "![AUCROC_Correlation](../_img/metrics.png \"AUCROC Correlation\")\n",
    "\n",
    "\n",
    "Running the same script (only changing **-alg XXX**), average performance on the validation data using other algorithms:\n",
    "\n",
    "| Alg  \t| F1  \t| AUC-ROC  \t|\n",
    "|---\t|---\t|---\t|\n",
    "| RF  \t| 0.787  \t| 0.824  \t|\n",
    "| SVMpoly  \t| 0.833  \t| 0.897  \t|\n",
    "| SVMrbf  \t| 0.855  \t| 0.905  \t|\n",
    "| SVM  \t| 0.856  \t| 0.911  \t|\n",
    "\n",
    "\n",
    "***SVM performed best on the validation data so we will continue with that algorithm!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Step 5](../_img/step5.png \"ML Workflow step 5\")\n",
    "\n",
    "Now that we have our best performing algorithm, we will run the pipeline one more time, but with more replicates (note, I still just use 10 here for time!) and we will use it to predict our unknown genes. \n",
    "\n",
    "**Additional input:**\n",
    "```\n",
    "- apply: List of lable names to apply trained model to (i.e. all, or 'unknown')\n",
    "- plots: True/False if you want the pipeline to generate performance metric plots (default = F)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -alg SVM \\\n",
    "                            -cv 5 \\\n",
    "                            -n 10 \\\n",
    "                            -apply unknown \\\n",
    "                            -plots T \\\n",
    "                            -save metab_SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check out our results...**\n",
    "\n",
    "Here are the files that are output from the model:\n",
    "- **data.txt_results:** A detailed look at the model that was run and its performance.  \n",
    "\n",
    "- **data.txt_scores:** The probability score for each gene (i.e. how confidently it was predicted) and the final classification for each gene, including the unknowns the model was applied to.\n",
    "\n",
    "- **data.txt_imp:** The importance of each feature in your model.\n",
    "\n",
    "- **data.txt_GridSearch:** Detailed results from the parameter grid search.\n",
    "\n",
    "- **data.txt_BalancedID:** A list of the genes that were included in each replicate after downsampling to balance the model.\n",
    "\n",
    "*For a detailed description of the content of the pipeline output see the [README](../README.md)*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we use fewer features?\n",
    "\n",
    "Additional input:\n",
    "```\n",
    "- feat: List of features to use.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -alg SVM \\\n",
    "                            -cv 5 \\\n",
    "                            -n 10 \\\n",
    "                            -feat top_feat_lasso.txt \\\n",
    "                            -save metab_SVM_lasso10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt \\\n",
    "                            -test test_genes.txt \\\n",
    "                            -cl_train special,gen \\\n",
    "                            -alg SVM \\\n",
    "                            -cv 5 \\\n",
    "                            -n 10 \\\n",
    "                            -feat rand_feat.txt_11 \\\n",
    "                            -save metab_SVM_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Results\n",
    "\n",
    "There are a number of vizualization tools available in the ML-Pipeline (see ML_Postprocessing). Here we will use ML_plots. \n",
    "\n",
    "\n",
    "**ML_plots.py input:**\n",
    "```\n",
    "-save: Name to save output figures\n",
    "-cl_train: positive and negative classes\n",
    "-names: short names to call each model being included\n",
    "-scores: path to name_scores.txt files to include\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts_PostAnalysis/ML_plots.py -save compare_SVM \\\n",
    "                    -cl_train special gen \\\n",
    "                    -names All LASSO Random \\\n",
    "                    -scores metab_SVM_scores.txt metab_SVM_lasso10_scores.txt metab_SVM_rand_scores.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Here we went through one example of how the ML pipeline can be used to automate a machine learning experiment. There are numerous advanced features included into the pipeline that were not covered. Run any script in the pipeline with no arguments (i.e. -arg) to see more options. \n",
    "\n",
    "**Some advanced options include:**\n",
    "- multi-class classification\n",
    "- transfer learning\n",
    "- comparing instance classification across models \n",
    "- single feature ML models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
